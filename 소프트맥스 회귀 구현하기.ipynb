{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22fe00b2430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =[[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
    "y_train = [2,2,2,1,1,1,0,0]\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros((4,3),requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([w,b],lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 6000 Cost : 0.399290\n",
      "Epoch  100 / 6000 Cost : 0.365709\n",
      "Epoch  200 / 6000 Cost : 0.333016\n",
      "Epoch  300 / 6000 Cost : 0.306667\n",
      "Epoch  400 / 6000 Cost : 0.294459\n",
      "Epoch  500 / 6000 Cost : 0.285098\n",
      "Epoch  600 / 6000 Cost : 0.276318\n",
      "Epoch  700 / 6000 Cost : 0.268061\n",
      "Epoch  800 / 6000 Cost : 0.260277\n",
      "Epoch  900 / 6000 Cost : 0.252923\n",
      "Epoch 1000 / 6000 Cost : 0.245962\n",
      "Epoch 1100 / 6000 Cost : 0.239362\n",
      "Epoch 1200 / 6000 Cost : 0.233094\n",
      "Epoch 1300 / 6000 Cost : 0.227132\n",
      "Epoch 1400 / 6000 Cost : 0.221454\n",
      "Epoch 1500 / 6000 Cost : 0.216040\n",
      "Epoch 1600 / 6000 Cost : 0.210872\n",
      "Epoch 1700 / 6000 Cost : 0.205932\n",
      "Epoch 1800 / 6000 Cost : 0.201206\n",
      "Epoch 1900 / 6000 Cost : 0.196680\n",
      "Epoch 2000 / 6000 Cost : 0.192342\n",
      "Epoch 2100 / 6000 Cost : 0.188180\n",
      "Epoch 2200 / 6000 Cost : 0.184184\n",
      "Epoch 2300 / 6000 Cost : 0.180344\n",
      "Epoch 2400 / 6000 Cost : 0.176652\n",
      "Epoch 2500 / 6000 Cost : 0.173098\n",
      "Epoch 2600 / 6000 Cost : 0.169677\n",
      "Epoch 2700 / 6000 Cost : 0.166380\n",
      "Epoch 2800 / 6000 Cost : 0.163201\n",
      "Epoch 2900 / 6000 Cost : 0.160134\n",
      "Epoch 3000 / 6000 Cost : 0.157174\n",
      "Epoch 3100 / 6000 Cost : 0.154314\n",
      "Epoch 3200 / 6000 Cost : 0.151550\n",
      "Epoch 3300 / 6000 Cost : 0.148878\n",
      "Epoch 3400 / 6000 Cost : 0.146293\n",
      "Epoch 3500 / 6000 Cost : 0.143791\n",
      "Epoch 3600 / 6000 Cost : 0.141368\n",
      "Epoch 3700 / 6000 Cost : 0.139021\n",
      "Epoch 3800 / 6000 Cost : 0.136746\n",
      "Epoch 3900 / 6000 Cost : 0.134540\n",
      "Epoch 4000 / 6000 Cost : 0.132400\n",
      "Epoch 4100 / 6000 Cost : 0.130324\n",
      "Epoch 4200 / 6000 Cost : 0.128308\n",
      "Epoch 4300 / 6000 Cost : 0.126350\n",
      "Epoch 4400 / 6000 Cost : 0.124448\n",
      "Epoch 4500 / 6000 Cost : 0.122599\n",
      "Epoch 4600 / 6000 Cost : 0.120801\n",
      "Epoch 4700 / 6000 Cost : 0.119053\n",
      "Epoch 4800 / 6000 Cost : 0.117352\n",
      "Epoch 4900 / 6000 Cost : 0.115696\n",
      "Epoch 5000 / 6000 Cost : 0.114084\n",
      "Epoch 5100 / 6000 Cost : 0.112514\n",
      "Epoch 5200 / 6000 Cost : 0.110985\n",
      "Epoch 5300 / 6000 Cost : 0.109495\n",
      "Epoch 5400 / 6000 Cost : 0.108042\n",
      "Epoch 5500 / 6000 Cost : 0.106626\n",
      "Epoch 5600 / 6000 Cost : 0.105244\n",
      "Epoch 5700 / 6000 Cost : 0.103896\n",
      "Epoch 5800 / 6000 Cost : 0.102581\n",
      "Epoch 5900 / 6000 Cost : 0.101296\n",
      "Epoch 6000 / 6000 Cost : 0.100043\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 6000\n",
    "\n",
    "for epoch in range(nb_epoch + 1):\n",
    "    hypothesis = F.softmax(x_train.matmul(w) + b,dim = 1)\n",
    "    \n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d} / {} Cost : {:.6f}'.format(epoch,nb_epoch,cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros((4,3),requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([w,b],lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 6000 Cost : 1.098612\n",
      "Epoch  100 / 6000 Cost : 0.761050\n",
      "Epoch  200 / 6000 Cost : 0.689991\n",
      "Epoch  300 / 6000 Cost : 0.643229\n",
      "Epoch  400 / 6000 Cost : 0.604117\n",
      "Epoch  500 / 6000 Cost : 0.568255\n",
      "Epoch  600 / 6000 Cost : 0.533922\n",
      "Epoch  700 / 6000 Cost : 0.500291\n",
      "Epoch  800 / 6000 Cost : 0.466908\n",
      "Epoch  900 / 6000 Cost : 0.433507\n",
      "Epoch 1000 / 6000 Cost : 0.399962\n",
      "Epoch 1100 / 6000 Cost : 0.366378\n",
      "Epoch 1200 / 6000 Cost : 0.333642\n",
      "Epoch 1300 / 6000 Cost : 0.307042\n",
      "Epoch 1400 / 6000 Cost : 0.294653\n",
      "Epoch 1500 / 6000 Cost : 0.285279\n",
      "Epoch 1600 / 6000 Cost : 0.276489\n",
      "Epoch 1700 / 6000 Cost : 0.268222\n",
      "Epoch 1800 / 6000 Cost : 0.260429\n",
      "Epoch 1900 / 6000 Cost : 0.253066\n",
      "Epoch 2000 / 6000 Cost : 0.246098\n",
      "Epoch 2100 / 6000 Cost : 0.239491\n",
      "Epoch 2200 / 6000 Cost : 0.233216\n",
      "Epoch 2300 / 6000 Cost : 0.227248\n",
      "Epoch 2400 / 6000 Cost : 0.221565\n",
      "Epoch 2500 / 6000 Cost : 0.216146\n",
      "Epoch 2600 / 6000 Cost : 0.210973\n",
      "Epoch 2700 / 6000 Cost : 0.206028\n",
      "Epoch 2800 / 6000 Cost : 0.201298\n",
      "Epoch 2900 / 6000 Cost : 0.196768\n",
      "Epoch 3000 / 6000 Cost : 0.192427\n",
      "Epoch 3100 / 6000 Cost : 0.188261\n",
      "Epoch 3200 / 6000 Cost : 0.184262\n",
      "Epoch 3300 / 6000 Cost : 0.180419\n",
      "Epoch 3400 / 6000 Cost : 0.176724\n",
      "Epoch 3500 / 6000 Cost : 0.173168\n",
      "Epoch 3600 / 6000 Cost : 0.169744\n",
      "Epoch 3700 / 6000 Cost : 0.166445\n",
      "Epoch 3800 / 6000 Cost : 0.163264\n",
      "Epoch 3900 / 6000 Cost : 0.160195\n",
      "Epoch 4000 / 6000 Cost : 0.157232\n",
      "Epoch 4100 / 6000 Cost : 0.154370\n",
      "Epoch 4200 / 6000 Cost : 0.151604\n",
      "Epoch 4300 / 6000 Cost : 0.148931\n",
      "Epoch 4400 / 6000 Cost : 0.146344\n",
      "Epoch 4500 / 6000 Cost : 0.143840\n",
      "Epoch 4600 / 6000 Cost : 0.141416\n",
      "Epoch 4700 / 6000 Cost : 0.139067\n",
      "Epoch 4800 / 6000 Cost : 0.136791\n",
      "Epoch 4900 / 6000 Cost : 0.134583\n",
      "Epoch 5000 / 6000 Cost : 0.132442\n",
      "Epoch 5100 / 6000 Cost : 0.130365\n",
      "Epoch 5200 / 6000 Cost : 0.128348\n",
      "Epoch 5300 / 6000 Cost : 0.126389\n",
      "Epoch 5400 / 6000 Cost : 0.124485\n",
      "Epoch 5500 / 6000 Cost : 0.122636\n",
      "Epoch 5600 / 6000 Cost : 0.120837\n",
      "Epoch 5700 / 6000 Cost : 0.119087\n",
      "Epoch 5800 / 6000 Cost : 0.117385\n",
      "Epoch 5900 / 6000 Cost : 0.115729\n",
      "Epoch 6000 / 6000 Cost : 0.114116\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 6000\n",
    "\n",
    "for epoch in range(nb_epoch + 1):\n",
    "    \n",
    "    z = x_train.matmul(w) + b\n",
    "    cost = F.cross_entropy(z,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d} / {} Cost : {:.6f}'.format(epoch,nb_epoch,cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 6000 Cost : 1.616785\n",
      "Epoch  100 / 6000 Cost : 0.658891\n",
      "Epoch  200 / 6000 Cost : 0.573443\n",
      "Epoch  300 / 6000 Cost : 0.518151\n",
      "Epoch  400 / 6000 Cost : 0.473265\n",
      "Epoch  500 / 6000 Cost : 0.433516\n",
      "Epoch  600 / 6000 Cost : 0.396563\n",
      "Epoch  700 / 6000 Cost : 0.360914\n",
      "Epoch  800 / 6000 Cost : 0.325392\n",
      "Epoch  900 / 6000 Cost : 0.289179\n",
      "Epoch 1000 / 6000 Cost : 0.254148\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(4,3)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.cross_entropy(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d} / {} Cost : {:.6f}'.format(epoch,nb_epoch,cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 10000 Cost : 0.020581\n",
      "Epoch 10000 / 10000 Cost : 0.013719\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr = 0.1)\n",
    "\n",
    "nb_epochs = 10000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.cross_entropy(prediction,y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10000 == 0:\n",
    "        print('Epoch {:4d} / {} Cost : {:.6f}'.format(epoch,nb_epochs,cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다 : cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "print(\"다음 기기로 학습합니다 :\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "    \n",
    "training_epochs = 30\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d345793953f4b579bda41b266b48da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84526f438be8440490c8e5064cef3bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c842f064ebf441739164a37109f424d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d214654c49034397ae65cfecdbe6fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dsets.MNIST(root = 'MNIST_data/',train = True, transform = transforms.ToTensor(),download = True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root = 'MNIST_data/',train = False, transform = transforms.ToTensor(),download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset =mnist_train, batch_size = batch_size,shuffle = True,drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.536267519\n",
      "Epoch: 0002 cost = 0.358625889\n",
      "Epoch: 0003 cost = 0.330814183\n",
      "Epoch: 0004 cost = 0.316112041\n",
      "Epoch: 0005 cost = 0.306813151\n",
      "Epoch: 0006 cost = 0.299845397\n",
      "Epoch: 0007 cost = 0.294825405\n",
      "Epoch: 0008 cost = 0.290664494\n",
      "Epoch: 0009 cost = 0.287341297\n",
      "Epoch: 0010 cost = 0.284192026\n",
      "Epoch: 0011 cost = 0.281761825\n",
      "Epoch: 0012 cost = 0.279679626\n",
      "Epoch: 0013 cost = 0.277516603\n",
      "Epoch: 0014 cost = 0.275792509\n",
      "Epoch: 0015 cost = 0.274091214\n",
      "Learing\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(784,10,bias = True).to(device)\n",
    "\n",
    "criterion =nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr = 0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for x,y in data_loader:\n",
    "        \n",
    "        x = x.view(-1,28*28).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(x)\n",
    "        cost = criterion(hypothesis,y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "    print('Epoch:','%04d' % (epoch + 1),'cost =','{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8859000205993652\n",
      "7302\n",
      "Label : 8\n",
      "Prediction :  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = linear(x_test)\n",
    "    correct_prediction = torch.argmax(prediction,1) == y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:',accuracy.item())\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test) - 1)\n",
    "    print(r)\n",
    "    x_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float().to(device)\n",
    "    y_single_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label :',y_single_data.item())\n",
    "    single_prediction = linear(x_single_data)\n",
    "    print('Prediction : ',torch.argmax(single_prediction,1).item())\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
